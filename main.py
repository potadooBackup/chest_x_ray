# -*- coding: utf-8 -*-
"""chest_x_ray.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jLTc79irBBuafsTVp5hn7LJMEcRhGZpB
"""
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision 
from torchvision import datasets
import torchvision.transforms as transforms
import os
import matplotlib.pyplot as plt
import seaborn as sns
from torchvision import models
#from tqdm import tqdm_notebook as tqdm
import time
from tqdm import tqdm
import warnings
import copy
warnings.simplefilter("ignore")
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
from torchsummary import summary
from sklearn.metrics import accuracy_score,classification_report, f1_score,roc_auc_score

#!pip install timm
#pre-trained models
import timm
#from efficientnet_pytorch import EfficientNet

#!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi
#!pip install gputil
#!pip install psutil
#!pip install humanize
import psutil
import humanize
import os
import GPUtil as GPU
GPUs = GPU.getGPUs()
gpu = GPUs[0]

def printGPU():
  process = psutil.Process(os.getpid())
  print("Gen RAM Free: " + humanize.naturalsize( psutil.virtual_memory().available ), " | Proc size: " + humanize.naturalsize( process.memory_info().rss))
  print("GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))
  
printGPU()

#!nvidia-smi

def images_transforms(phase):
    if phase == 'training':
        data_transformation =transforms.Compose([
            transforms.Resize(IMAGE_SIZE),
            #transforms.RandomEqualize(10),
            transforms.RandomRotation(degrees=(-20,20)),
            transforms.RandomHorizontalFlip(p=0.5),
            #transforms.CenterCrop(192),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])
        ])
    else:
        data_transformation=transforms.Compose([
            transforms.Resize(IMAGE_SIZE),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])
        ])
        
    
    return data_transformation

class ResNet152(nn.Module):
    def __init__(self,num_class,pretrained_option=False):
        super(ResNet152,self).__init__()
        self.model=models.resnet152(pretrained=pretrained_option)
        
        if pretrained_option==True:
            for param in self.model.parameters():
                param.requires_grad=False

        num_neurons=self.model.fc.in_features
        self.model.fc= nn.Linear(num_neurons,num_class)
    def forward(self,X):
        out=self.model(X)
        return out

class EfficientNet(nn.Module):
    def __init__(self,num_class,pretrained_option=False):
        super(EfficientNet,self).__init__()
        self.model=timm.create_model('tf_efficientnet_b4_ns',pretrained=True)
        
        if pretrained_option==True:
            for param in self.model.parameters():
                param.requires_grad=False
        
        num_neurons=self.model.classifier.in_features
        self.model.classifier=nn.Linear(num_neurons,num_class)
        
    def forward(self,X):
        out=self.model(X)
        return out

def training(model, train_loader, test_loader, Loss, optimizer, epochs, device, num_class, name):
    model.to(device)
    best_model_wts = None
    best_evaluated_acc = 0
    train_acc = []
    test_acc = []
    test_Recall = []
    test_Precision = []
    test_F1_score = []
    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer , gamma = 0.9)
    for epoch in range(1, epochs+1):
        with torch.set_grad_enabled(True):
            model.train()
            total_loss=0
            correct=0
            for idx,(data, label) in enumerate(tqdm(train_loader)):
                optimizer.zero_grad()
                        
                data = data.to(device,dtype=torch.float)
                label = label.to(device,dtype=torch.long)

                predict = model(data)      

                loss = Loss(predict, label.squeeze())

                total_loss += loss.item()
                pred = torch.max(predict,1).indices
                correct += pred.eq(label).cpu().sum().item()
                
                loss.backward()
                optimizer.step()

            total_loss /= len(train_loader.dataset)
            correct = (correct/len(train_loader.dataset))*100.
            print ("Epoch : " , epoch)
            print ("Loss : " , total_loss)
            print ("Correct : " , correct)
            #print(epoch, total_loss, correct)     
        scheduler.step()
        accuracy  , Recall , Precision , F1_score, _ = evaluate(model, device, test_loader)
        train_acc.append(correct)  
        test_acc.append(accuracy)
        test_Recall.append(Recall)
        test_Precision.append(Precision)
        test_F1_score.append(F1_score)

        if accuracy > best_evaluated_acc:
            best_evaluated_acc = accuracy
            best_model_wts = copy.deepcopy(model.state_dict())
    #save model
    torch.save(best_model_wts, name+".pt")
    model.load_state_dict(best_model_wts)

    return train_acc , test_acc , test_Recall , test_Precision , test_F1_score

def evaluate(model, device, test_loader):
    correct=0
    TP=0
    TN=0
    FP=0
    FN=0
    with torch.set_grad_enabled(False):
        model.eval()
        for idx,(data,label) in enumerate(test_loader):
            data = data.to(device,dtype=torch.float)
            label = label.to(device,dtype=torch.long)
            predict = model(data)
            pred = torch.max(predict,1).indices
            #correct += pred.eq(label).cpu().sum().item()
            for j in range(data.size()[0]):
                #print ("{} pred label: {} ,true label:{}" .format(len(pred),pred[j],int(label[j])))
                if (int (pred[j]) == int (label[j])):
                    correct +=1
                if (int (pred[j]) == 1 and int (label[j]) ==  1):
                    TP += 1
                if (int (pred[j]) == 0 and int (label[j]) ==  0):
                    TN += 1
                if (int (pred[j]) == 1 and int (label[j]) ==  0):
                    FP += 1
                if (int (pred[j]) == 0 and int (label[j]) ==  1):
                    FN += 1
        print ("TP : " , TP)
        print ("TN : " , TN)
        print ("FP : " , FP)
        print ("FN : " , FN)

        print ("num_correct :",correct ," / " , len(test_loader.dataset))
        Recall = TP/(TP+FN)
        print ("Recall : " ,  Recall )

        Precision = TP/(TP+FP)
        print ("Preecision : " ,  Precision )

        F1_score = 2 * Precision * Recall / (Precision + Recall)
        print ("F1 - score : " , F1_score)

        correct = (correct/len(test_loader.dataset))*100.
        print ("Accuracy : " , correct ,"%")

    return correct , Recall , Precision , F1_score, [[TN,FP],[FN,TP]]

IMAGE_SIZE=(192,192)
batch_size=128
learning_rate = 0.01
epochs=16
num_classes=2

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print (device)

train_path='/content/drive/MyDrive/chest_xray/train'
test_path='/content/drive/MyDrive/chest_xray/test'
val_path='/content/drive/MyDrive/chest_xray/val'

trainset=datasets.ImageFolder(train_path,transform=images_transforms('training'))
testset=datasets.ImageFolder(test_path,transform=images_transforms('test'))
valset=datasets.ImageFolder(val_path,transform=images_transforms('val'))

train_loader = DataLoader(trainset,batch_size=batch_size,shuffle=True,num_workers=2)
test_loader = DataLoader(testset,batch_size=batch_size,shuffle=True,num_workers=2)
val_loader = DataLoader(valset,batch_size=batch_size,shuffle=True,num_workers=2)

examples=iter(train_loader)
images,labels=examples.next()
print(images.shape)
# imshow(torchvision.utils.make_grid(images[:56],pad_value=20))

#model = torch.load('90_CNN_chest.pt')
model = ResNet152(2, True)
#model = EfficientNet(num_classes, True)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# print (summary(model,(3,128,128)))

print (train_loader)
dataiter = iter(train_loader)
images , labels = dataiter.next()
print (type(images) , type(labels))
print (images.size(),labels.size())

Loss = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
train_acc , test_acc , test_Recall , test_Precision , test_F1_score  = training(model, train_loader, test_loader, Loss, optimizer, epochs, device, num_classes, 'CNN_chest')

print("Best Result:")
accuracy  , Recall , Precision , F1_score, heatmap_data = evaluate(model, device, test_loader)

x_axis = [i for i in range(1,1+epochs)]
plt.title("Accuracy") # title
plt.ylabel("%") # y label
plt.xlabel("epochs") # x label
plt.plot(x_axis, train_acc, color = 'red', marker = 'o', label = 'train_acc')
plt.plot(x_axis, test_acc, color = 'blue', marker = 'o', label = 'test_acc')
plt.legend()
plt.show()

plt.title("test-F1-Score") # title
plt.ylabel("%") # y label
plt.xlabel("epochs") # x label
plt.plot(x_axis, test_F1_score, color = 'red', marker = 'o', label = 'train_acc')
plt.show()

sns.heatmap(heatmap_data, annot = True, cmap = 'YlOrRd', fmt='.3g')
plt.xlabel('Actual') 
plt.ylabel('Prediction')


def saliency(model):
    inv_normalize = transforms.Normalize(
        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],
        std=[1/0.229, 1/0.224, 1/0.255]
    )
    #we don't need gradients w.r.t. weights for a trained model
    for param in model.parameters():
        param.requires_grad = False
    
    #set model in eval mode
    model.eval()
    
    examples=iter(test_loader)
    images,labels=examples.next()
    input = images[:1]
    input = input.to(device,dtype=torch.float)

    #we want to calculate gradient of higest score w.r.t. input
    #so set requires_grad to True for input 
    input.requires_grad = True
    #forward pass to calculate predictions
    preds = model(input)
    score, indices = torch.max(preds, 1)
    #backward pass to get gradients of score predicted class w.r.t. input image
    score.backward()
    #get max along channel axis
    slc, _ = torch.max(torch.abs(input.grad[0]), dim=0)
    #normalize to [0..1]
    slc = (slc - slc.min())/(slc.max()-slc.min())

    #apply inverse transform on image
    with torch.no_grad():
        input_img = inv_normalize(input[0])
    #plot image and its saleincy map
    plt.figure(figsize=(10, 10))
    plt.subplot(1, 2, 1)
    plt.imshow(np.transpose(input_img.cpu().detach().numpy(), (1, 2, 0)))
    plt.xticks([])
    plt.yticks([])
    plt.subplot(1, 2, 2)
    plt.imshow(slc.cpu().numpy(), cmap=plt.cm.hot)
    plt.xticks([])
    plt.yticks([])
    plt.show()

saliency(model)
saliency(model)